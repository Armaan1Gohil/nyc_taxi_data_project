{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5fee2b7-2e20-4971-ab1c-0398ad9f4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2edb9412-96d8-4a33-8fdf-3edc7a326fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --year YEAR --month MONTH\n",
      "ipykernel_launcher.py: error: the following arguments are required: --year, --month\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Datetime data for transformation')\n",
    "parser.add_argument('--year', type=int, required=True)\n",
    "parser.add_argument('--month', type=int, required=True)\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "986e8379-26b8-473f-881d-24ddff425678",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m year \u001b[38;5;241m=\u001b[39m \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39myear\n\u001b[1;32m      2\u001b[0m month \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mmonth\n\u001b[1;32m      3\u001b[0m catalog \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnessie\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "year = args.year\n",
    "month = args.month\n",
    "catalog = 'nessie'\n",
    "namespace = 'nyc_project_db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca077e71-5b78-48a1-ba2b-02b074b8e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = 's3a://nyc-project/raw-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff620977-e0a2-4e99-bf5f-5798da462433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/24 17:51:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('data_transformation') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a226243b-6d92-4e9e-9212-c3136a5c5e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new schema with less memory formats\n",
    "new_schema = types.StructType([\n",
    "    types.StructField('VendorID', types.IntegerType(), True), \n",
    "    types.StructField('tpep_pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('tpep_dropoff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('passenger_count', types.IntegerType(), True), \n",
    "    types.StructField('trip_distance', types.FloatType(), True), \n",
    "    types.StructField('RatecodeID', types.IntegerType(), True), \n",
    "    types.StructField('store_and_fwd_flag', types.StringType(), True), \n",
    "    types.StructField('PULocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOLocationID', types.IntegerType(), True), \n",
    "    types.StructField('payment_type', types.IntegerType(), True), \n",
    "    types.StructField('fare_amount', types.FloatType(), True), \n",
    "    types.StructField('extra', types.FloatType(), True), \n",
    "    types.StructField('mta_tax', types.FloatType(), True), \n",
    "    types.StructField('tip_amount', types.FloatType(), True), \n",
    "    types.StructField('tolls_amount', types.FloatType(), True), \n",
    "    types.StructField('improvement_surcharge', types.FloatType(), True), \n",
    "    types.StructField('total_amount', types.FloatType(), True), \n",
    "    types.StructField('congestion_surcharge', types.FloatType(), True), \n",
    "    types.StructField('airport_fee', types.IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "481f32e2-722e-4f62-a49b-9afb884f3a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw data\n",
    "df = spark.read.format('parquet').load(f'{s3_path}/2019/01/*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "056b1c29-58e3-4173-80cb-a2f891013539",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_schema = df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "de8becd5-bf83-4106-bb2e-f697aa549c74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the dataframe in the new schema\n",
    "for old_field, new_field in zip(old_schema.fields, new_schema.fields):\n",
    "    df = df.withColumn(new_field.name, col(old_field.name).cast(new_field.dataType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ef0183e8-6fdf-46b7-94c6-debcd6fd3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename all the columns\n",
    "df = df.withColumnRenamed('VendorID', 'vendor_id') \\\n",
    "    .withColumnRenamed('RatecodeID', 'ratecode_id') \\\n",
    "    .withColumnRenamed('payment_type', 'payment_type_id') \\\n",
    "    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime') \\\n",
    "    .withColumnRenamed('PULocationID', 'pickup_location_id') \\\n",
    "    .withColumnRenamed('DOLocationID', 'dropoff_location_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "00275fbd-a65d-459c-b975-1e1bbdeb77fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove the data which doesn't make sense\n",
    "df = df.filter((col('fare_amount') > 0) \\\n",
    "               & (col('trip_distance') > 0) \\\n",
    "               & (col('extra') > 0))\n",
    "\n",
    "# Ratecode ID cannot be more than 6\n",
    "df = df.filter((col('ratecode_id') <= 6))\n",
    "\n",
    "# Replace all the Null values with 0\n",
    "df = df.withColumn('congestion_surcharge', F.when(col('congestion_surcharge').isNull(), 0).otherwise(col('congestion_surcharge')))\n",
    "df = df.withColumn('airport_fee', F.when(col('airport_fee').isNull(), 0).otherwise(col('airport_fee')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4b89f317-1f19-47f3-9098-69bfed75cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/15 16:18:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/15 16:18:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/15 16:19:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/15 16:19:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Repalce 0 passenger count with median\n",
    "\n",
    "window_spec = Window.orderBy('passenger_count')\n",
    "\n",
    "df_rn = df.select(['passenger_count']).withColumn('rn', F.row_number().over(window_spec))\n",
    "total_rows = df.count()\n",
    "                                                         \n",
    "if total_rows % 2 == 0:\n",
    "    lower_mid = total_rows // 2\n",
    "    upper_mid = lower_mid + 1\n",
    "else:\n",
    "    lower_mid = total_rows // 2 + 1\n",
    "    upper_mid = lower_mid\n",
    "\n",
    "median_df = df_rn.filter((col('rn') == lower_mid) | (col('rn') == upper_mid))\n",
    "\n",
    "median_value = median_df.agg(F.avg(col('passenger_count'))).collect()[0][0]\n",
    "\n",
    "df = df.withColumn('passenger_count', F.when(col('passenger_count') == 0, median_value).otherwise(col('passenger_count')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d686f27e-c657-462e-ac2c-6eceef3ca5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique ID on the basis of timestamp\n",
    "\n",
    "def index_id(date_column):\n",
    "    year = F.year(date_column)\n",
    "    month = F.lpad(F.month(date_column).cast(\"string\"), 2, \"0\")\n",
    "    day = F.lpad(F.dayofmonth(date_column).cast(\"string\"), 2, \"0\")\n",
    "    hour = F.lpad(F.hour(date_column).cast(\"string\"), 2, \"0\")\n",
    "    minute = F.lpad(F.minute(date_column).cast(\"string\"), 2, \"0\")\n",
    "    second = F.lpad(F.second(date_column).cast(\"string\"), 2, \"0\")\n",
    "    index = F.concat(year, month, day, hour, minute, second)\n",
    "    return index.cast('long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "395859b3-d247-4aac-8de8-c8123540e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dimension Table for Pickup Datetime\n",
    "pickup_datetime_dim = df.select(['pickup_datetime']) \\\n",
    "    .distinct() \\\n",
    "    .withColumn('pickup_datetime_id', index_id(col('pickup_datetime'))) \\\n",
    "    .withColumn('pickup_hour', F.hour(col('pickup_datetime'))) \\\n",
    "    .withColumn('pickup_day', F.dayofmonth(col('pickup_datetime'))) \\\n",
    "    .withColumn('pickup_month', F.month(col('pickup_datetime'))) \\\n",
    "    .withColumn('pickup_year', F.year(col('pickup_datetime'))) \\\n",
    "    .withColumn('pickup_weekday', F.date_format(col('pickup_datetime'), 'EEEE'))\n",
    "\n",
    "pickup_datetime_dim = pickup_datetime_dim.select(\n",
    "    'pickup_datetime_id',\n",
    "    'pickup_datetime',\n",
    "    'pickup_hour',\n",
    "    'pickup_day',\n",
    "    'pickup_month',\n",
    "    'pickup_year',\n",
    "    'pickup_weekday'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "16721413-43fe-448d-87ad-58de5d35d296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {catalog}.{namespace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8dec25df-cb31-4657-b4d5-076a3f0f9e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the new pickup datetime table\n",
    "\n",
    "columns_name = []\n",
    "pickup_datetime_schema = pickup_datetime_dim.schema\n",
    "for field in pickup_datetime_schema.fields:\n",
    "    columns_name.append(f'{field.name} {field.dataType.simpleString().upper()}')\n",
    "columns_sql = \", \".join(columns_name)\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {catalog}.{namespace}.pickup_datetime_table (\n",
    "    {columns_sql}\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (pickup_year)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0efd0fe0-9ef8-4442-af12-0707aff46476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Write the dataframe to Iceberg tables\n",
    "pickup_datetime_dim.write.format('iceberg') \\\n",
    "    .mode('overwrite') \\\n",
    "    .partitionBy('pickup_year') \\\n",
    "    .save(f'{catalog}.{namespace}.pickup_datetime_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0484d166-7561-4c65-8333-1c661841e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Dimension Table for Dropoff Datetime \n",
    "dropoff_datetime_dim = df.select(['dropoff_datetime']) \\\n",
    "    .distinct() \\\n",
    "    .withColumn('dropoff_datetime_id', index_id(col('dropoff_datetime'))) \\\n",
    "    .withColumn('dropoff_hour', F.hour(col('dropoff_datetime'))) \\\n",
    "    .withColumn('dropoff_day', F.dayofmonth(col('dropoff_datetime'))) \\\n",
    "    .withColumn('dropoff_month', F.month(col('dropoff_datetime'))) \\\n",
    "    .withColumn('dropoff_year', F.year(col('dropoff_datetime'))) \\\n",
    "    .withColumn('dropoff_weekday', F.date_format(col('dropoff_datetime'), 'EEEE'))\n",
    "\n",
    "dropoff_datetime_dim = dropoff_datetime_dim.select(\n",
    "    'dropoff_datetime_id',\n",
    "    'dropoff_datetime',\n",
    "    'dropoff_hour',\n",
    "    'dropoff_day',\n",
    "    'dropoff_month',\n",
    "    'dropoff_year',\n",
    "    'dropoff_weekday'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b9b464a3-68aa-49d7-9df4-d6a5ac140a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the new pickup datetime table\n",
    "\n",
    "columns_name = []\n",
    "dropoff_datetime_schema = dropoff_datetime_dim.schema\n",
    "for field in dropoff_datetime_schema.fields:\n",
    "    columns_name.append(f'{field.name} {field.dataType.simpleString().upper()}')\n",
    "columns_sql = \", \".join(columns_name)\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {catalog}.{namespace}.dropoff_datetime_table (\n",
    "    {columns_sql}\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (dropoff_year)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f3a7df09-8ead-4867-a7b3-28a1538338f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Write the dataframe to iceberg tables\n",
    "dropoff_datetime_dim.write.format('iceberg') \\\n",
    "    .mode('overwrite') \\\n",
    "    .partitionBy('dropoff_year') \\\n",
    "    .save(f'{catalog}.{namespace}.dropoff_datetime_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "04e810f4-0b1d-4a7f-a120-22dda76f64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the foreign key of dropp datetime table & pickup datetime table into fact tables\n",
    "df = df.withColumn('dropoff_datetime_id', index_id(col('dropoff_datetime'))) \\\n",
    "    .withColumn('pickup_datetime_id', index_id(col('pickup_datetime')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "43c66ecc-8eb5-4eb1-a67e-c7b09b5b2a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(['vendor_id',\n",
    " 'pickup_datetime_id',\n",
    " 'dropoff_datetime_id',\n",
    " 'pickup_location_id',\n",
    " 'dropoff_location_id',\n",
    " 'ratecode_id',\n",
    " 'passenger_count',\n",
    " 'trip_distance',\n",
    " 'payment_type_id',\n",
    " 'store_and_fwd_flag',\n",
    " 'fare_amount',\n",
    " 'extra',\n",
    " 'mta_tax',\n",
    " 'tip_amount',\n",
    " 'tolls_amount',\n",
    " 'improvement_surcharge',\n",
    " 'congestion_surcharge',\n",
    " 'airport_fee',\n",
    " 'total_amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "659e9fd1-80ff-4912-a054-f5e1f8ea839d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Fact Table in Iceberg\n",
    "\n",
    "columns_name = []\n",
    "df_schema = df.schema\n",
    "for field in df_schema.fields:\n",
    "    columns_name.append(f'{field.name} {field.dataType.simpleString().upper()}')\n",
    "columns_sql = \", \".join(columns_name)\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {catalog}.{namespace}.fact_table (\n",
    "    {columns_sql}\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "115132b4-0e9f-4780-8f70-b7a994fef492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Write the Fact Dataframe to Iceberg\n",
    "df.write.format('iceberg') \\\n",
    "    .mode('overwrite') \\\n",
    "    .save(f'{catalog}.{namespace}.fact_table')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
