{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5fee2b7-2e20-4971-ab1c-0398ad9f4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "986e8379-26b8-473f-881d-24ddff425678",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = sys.argv[1]\n",
    "month = sys.argv[2]\n",
    "catalogue = 'nessie'\n",
    "namespace = 'nyc_project_db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca077e71-5b78-48a1-ba2b-02b074b8e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = 's3a://nyc-project/raw-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff620977-e0a2-4e99-bf5f-5798da462433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/02 19:49:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('data_transformation') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d08222ee-c4fd-4782-aa30-0304f749a679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e185274d-3087-4676-9d21-49fe2a5bcbf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'minio_access_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize Spark session\u001b[39;00m\n\u001b[1;32m      4\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_transformation\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.access.key\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mminio_access_key\u001b[49m) \\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.secret.key\u001b[39m\u001b[38;5;124m\"\u001b[39m, minio_secret_key) \\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://object-storage:9000\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.path.style.access\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.connection.ssl.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.impl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.hadoop.fs.s3a.S3AFileSystem\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.my_catalog\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.iceberg.spark.SparkCatalog\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.my_catalog.type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhadoop\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.my_catalog.warehouse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3a://iceberg/warehouse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Create SparkContext\u001b[39;00m\n\u001b[1;32m     19\u001b[0m sc \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\n",
      "\u001b[0;31mNameError\u001b[0m: name 'minio_access_key' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('data_transformation') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", minio_access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", minio_secret_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://object-storage:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.catalog.my_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.my_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.my_catalog.warehouse\", \"s3a://iceberg/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a226243b-6d92-4e9e-9212-c3136a5c5e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_schema = types.StructType([\n",
    "    types.StructField('VendorID', types.IntegerType(), True), \n",
    "    types.StructField('tpep_pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('tpep_dropoff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('passenger_count', types.IntegerType(), True), \n",
    "    types.StructField('trip_distance', types.FloatType(), True), \n",
    "    types.StructField('RatecodeID', types.IntegerType(), True), \n",
    "    types.StructField('store_and_fwd_flag', types.StringType(), True), \n",
    "    types.StructField('PULocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOLocationID', types.IntegerType(), True), \n",
    "    types.StructField('payment_type', types.IntegerType(), True), \n",
    "    types.StructField('fare_amount', types.FloatType(), True), \n",
    "    types.StructField('extra', types.FloatType(), True), \n",
    "    types.StructField('mta_tax', types.FloatType(), True), \n",
    "    types.StructField('tip_amount', types.FloatType(), True), \n",
    "    types.StructField('tolls_amount', types.FloatType(), True), \n",
    "    types.StructField('improvement_surcharge', types.FloatType(), True), \n",
    "    types.StructField('total_amount', types.FloatType(), True), \n",
    "    types.StructField('congestion_surcharge', types.FloatType(), True), \n",
    "    types.StructField('airport_fee', types.IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "481f32e2-722e-4f62-a49b-9afb884f3a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/25 19:23:08 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('headers', True).parquet(f'{s3_path}/2019/01/*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "056b1c29-58e3-4173-80cb-a2f891013539",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_schema = df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8becd5-bf83-4106-bb2e-f697aa549c74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for old_field, new_field in zip(old_schema.fields, new_schema.fields):\n",
    "    df = df.withColumn(new_field.name, col(old_field.name).cast(new_field.dataType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef0183e8-6fdf-46b7-94c6-debcd6fd3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('VendorID', 'vendor_id') \\\n",
    "    .withColumnRenamed('RatecodeID', 'ratecode_id') \\\n",
    "    .withColumnRenamed('payment_type', 'payment_type_id') \\\n",
    "    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime') \\\n",
    "    .withColumnRenamed('PULocationID', 'pickup_location_id') \\\n",
    "    .withColumnRenamed('DOLocationID', 'dropoff_location_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00275fbd-a65d-459c-b975-1e1bbdeb77fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.filter((col('fare_amount') > 0) \\\n",
    "               & (col('trip_distance') > 0) \\\n",
    "               & (col('extra') > 0))\n",
    "\n",
    "df = df.filter((col('ratecode_id') <= 6))\n",
    "\n",
    "df = df.withColumn('congestion_surcharge', F.when(col('congestion_surcharge').isNull(), 0).otherwise(col('congestion_surcharge')))\n",
    "df = df.withColumn('airport_fee', F.when(col('airport_fee').isNull(), 0).otherwise(col('airport_fee')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b89f317-1f19-47f3-9098-69bfed75cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/25 19:23:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/11/25 19:23:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/11/25 19:23:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/11/25 19:23:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy('passenger_count')\n",
    "\n",
    "df_rn = df.select(['passenger_count']).withColumn('rn', F.row_number().over(window_spec))\n",
    "total_rows = df.count()\n",
    "                                                         \n",
    "if total_rows % 2 == 0:\n",
    "    lower_mid = total_rows // 2\n",
    "    upper_mid = lower_mid + 1\n",
    "else:\n",
    "    lower_mid = total_rows // 2 + 1\n",
    "    upper_mid = lower_mid\n",
    "\n",
    "median_df = df_rn.filter((col('rn') == lower_mid) | (col('rn') == upper_mid))\n",
    "\n",
    "median_value = median_df.agg(F.avg(col('passenger_count'))).collect()[0][0]\n",
    "\n",
    "df = df.withColumn('passenger_count', F.when(col('passenger_count') == 0, median_value).otherwise(col('passenger_count')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d686f27e-c657-462e-ac2c-6eceef3ca5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_id(date_column):\n",
    "    year = F.year(date_column)\n",
    "    month = F.lpad(F.month(date_column).cast(\"string\"), 2, \"0\")\n",
    "    day = F.lpad(F.dayofmonth(date_column).cast(\"string\"), 2, \"0\")\n",
    "    hour = F.lpad(F.hour(date_column).cast(\"string\"), 2, \"0\")\n",
    "    minute = F.lpad(F.minute(date_column).cast(\"string\"), 2, \"0\")\n",
    "    second = F.lpad(F.second(date_column).cast(\"string\"), 2, \"0\")\n",
    "    index = F.concat(year, month, day, hour, minute, second)\n",
    "    return index.cast(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "395859b3-d247-4aac-8de8-c8123540e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_datetime_dim = df.select(['pickup_datetime']) \\\n",
    "    .distinct() \\\n",
    "    .withColumn('pickup_datetime_id', index_id(col('pickup_datetime'))) \\\n",
    "    .withColumn('pickup_hour', F.hour(col('pickup_datetime'))) \\\n",
    "    .withColumn('pickup_day', F.dayofmonth(col('pickup_datetime'))) \\\n",
    "    .withColumn('pickup_month', F.month(col('pickup_datetime'))) \\\n",
    "    .withColumn('pickup_year', F.year(col('pickup_datetime'))) \\\n",
    "    .withColumn('pickup_weekday', F.date_format(col('pickup_datetime'), 'EEEE'))\n",
    "\n",
    "pickup_datetime_dim = pickup_datetime_dim.select(\n",
    "    'pickup_datetime_id',\n",
    "    'pickup_datetime',\n",
    "    'pickup_hour',\n",
    "    'pickup_day',\n",
    "    'pickup_month',\n",
    "    'pickup_year',\n",
    "    'pickup_weekday'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16721413-43fe-448d-87ad-58de5d35d296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {catalogue}.{namespace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dec25df-cb31-4657-b4d5-076a3f0f9e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_name = []\n",
    "pickup_datetime_schema = pickup_datetime_dim.schema\n",
    "for field in pickup_datetime_schema.fields:\n",
    "    columns_name.append(f'{field.name} {field.dataType.simpleString().upper()}')\n",
    "columns_sql = \", \".join(columns_name)\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {catalogue}.{namespace}.pickup_datetime_table (\n",
    "    {columns_sql}\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0efd0fe0-9ef8-4442-af12-0707aff46476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pickup_datetime_dim.writeTo(f'{catalogue}.{namespace}.pickup_datetime_table').createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0484d166-7561-4c65-8333-1c661841e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoff_datetime_dim = df.select(['dropoff_datetime']) \\\n",
    "    .distinct() \\\n",
    "    .withColumn('dropoff_datetime_id', index_id(col('dropoff_datetime'))) \\\n",
    "    .withColumn('dropoff_hour', F.hour(col('dropoff_datetime'))) \\\n",
    "    .withColumn('dropoff_day', F.dayofmonth(col('dropoff_datetime'))) \\\n",
    "    .withColumn('dropoff_month', F.month(col('dropoff_datetime'))) \\\n",
    "    .withColumn('dropoff_year', F.year(col('dropoff_datetime'))) \\\n",
    "    .withColumn('dropoff_weekday', F.date_format(col('dropoff_datetime'), 'EEEE'))\n",
    "\n",
    "dropoff_datetime_dim = dropoff_datetime_dim.select(\n",
    "    'dropoff_datetime_id',\n",
    "    'dropoff_datetime',\n",
    "    'dropoff_hour',\n",
    "    'dropoff_day',\n",
    "    'dropoff_month',\n",
    "    'dropoff_year',\n",
    "    'dropoff_weekday'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9b464a3-68aa-49d7-9df4-d6a5ac140a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_name = []\n",
    "dropoff_datetime_schema = dropoff_datetime_dim.schema\n",
    "for field in dropoff_datetime_schema.fields:\n",
    "    columns_name.append(f'{field.name} {field.dataType.simpleString().upper()}')\n",
    "columns_sql = \", \".join(columns_name)\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {catalogue}.{namespace}.dropoff_datetime_table (\n",
    "    {columns_sql}\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3a7df09-8ead-4867-a7b3-28a1538338f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dropoff_datetime_dim.writeTo(f'{catalogue}.{namespace}.dropoff_datetime_table').createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04e810f4-0b1d-4a7f-a120-22dda76f64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('dropoff_datetime_id', index_id(col('dropoff_datetime'))) \\\n",
    "    .withColumn('pickup_datetime_id', index_id(col('pickup_datetime')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43c66ecc-8eb5-4eb1-a67e-c7b09b5b2a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(['vendor_id',\n",
    " 'pickup_datetime_id',\n",
    " 'dropoff_datetime_id',\n",
    " 'pickup_location_id',\n",
    " 'dropoff_location_id',\n",
    " 'ratecode_id',\n",
    " 'passenger_count',\n",
    " 'trip_distance',\n",
    " 'payment_type_id',\n",
    " 'store_and_fwd_flag',\n",
    " 'fare_amount',\n",
    " 'extra',\n",
    " 'mta_tax',\n",
    " 'tip_amount',\n",
    " 'tolls_amount',\n",
    " 'improvement_surcharge',\n",
    " 'congestion_surcharge',\n",
    " 'airport_fee',\n",
    " 'total_amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "659e9fd1-80ff-4912-a054-f5e1f8ea839d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_name = []\n",
    "df_schema = df.schema\n",
    "for field in df_schema.fields:\n",
    "    columns_name.append(f'{field.name} {field.dataType.simpleString().upper()}')\n",
    "columns_sql = \", \".join(columns_name)\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {catalogue}.{namespace}.fact_table (\n",
    "    {columns_sql}\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "115132b4-0e9f-4780-8f70-b7a994fef492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.writeTo(f'{catalogue}.{namespace}.fact_table').createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dbfa4a6-f92d-4d51-8ddd-5fd6341bf905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----------+\n",
      "|     namespace|           tableName|isTemporary|\n",
      "+--------------+--------------------+-----------+\n",
      "|nyc_project_db|dropoff_datetime_...|      false|\n",
      "|nyc_project_db|          fact_table|      false|\n",
      "|nyc_project_db|pickup_datetime_t...|      false|\n",
      "+--------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN nessie.nyc_project_db\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b211eb69-773a-4f1c-95ea-f66f4823c96d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[SCHEMA_NOT_FOUND] The schema `my_namespace` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\nTo tolerate the error on drop use DROP SCHEMA IF EXISTS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDROP SCHEMA nessie.my_namespace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [SCHEMA_NOT_FOUND] The schema `my_namespace` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\nTo tolerate the error on drop use DROP SCHEMA IF EXISTS."
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP SCHEMA nessie.my_namespace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52889f4d-ac53-4acf-8085-c2e1a9486d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     namespace|\n",
      "+--------------+\n",
      "|nyc_project_db|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW NAMESPACES IN nessie\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bdc726e-6702-498f-86d9-2fb06ddb1d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 19:03:49 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-------------------+------------------+-------------------+-----------+---------------+-------------+---------------+------------------+-----------+-----+-------+----------+------------+---------------------+--------------------+-----------+------------+\n",
      "|vendor_id|pickup_datetime_id|dropoff_datetime_id|pickup_location_id|dropoff_location_id|ratecode_id|passenger_count|trip_distance|payment_type_id|store_and_fwd_flag|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|congestion_surcharge|airport_fee|total_amount|\n",
      "+---------+------------------+-------------------+------------------+-------------------+-----------+---------------+-------------+---------------+------------------+-----------+-----+-------+----------+------------+---------------------+--------------------+-----------+------------+\n",
      "|        1|              null|               null|               151|                239|          1|            1.0|          1.5|              1|                 N|        7.0|  0.5|    0.5|      1.65|         0.0|                  0.3|                 0.0|          0|        9.95|\n",
      "|        1|              null|               null|               239|                246|          1|            1.0|          2.6|              1|                 N|       14.0|  0.5|    0.5|       1.0|         0.0|                  0.3|                 0.0|          0|        16.3|\n",
      "+---------+------------------+-------------------+------------------+-------------------+-----------+---------------+-------------+---------------+------------------+-----------+-----+-------+----------+------------+---------------------+--------------------+-----------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM nessie.nyc_project_db.fact_table\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71eb2d44-55d7-4544-a6b2-dc436deded46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|       nessie|\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f43525d-3a99-442b-ba2b-e782d2bf12cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
